{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 4: Missing Data and Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Natesh Pillai\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instructions\"></a>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please **restart the kernel and run the entire notebook again before you submit.**\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code continues to work, restart the kernel and rerun your notebook periodically while working through this assignment. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use `.head(...)` when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate and report $R^2$\", do not just output the value from a cell. Write a `print(...)` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- **Your plots MUST be clearly labeled and easy to read,** including clear labels for the $x$ and $y$ axes, a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients for degree-5 polynomial model\" on the other hand is descriptive), a legend when appropriate, and clearly formatted text and graphics.\n",
    "\n",
    "- **Your code may also be evaluated for efficiency and clarity.** As a result, correct output is not always sufficient for full credit.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3c67b69c-c35a-45ee-88ff-99c701edb0a0",
    "colab_type": "text",
    "id": "BlViDCbxVtbG"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**PART 1 [55 pts]: Predicting the selling price of used cars**](#part1)\n",
    "  - [Overview and Data Description](#part1intro)\n",
    "  - [Question 1: Visualizing Missing Data [10 pts]](#part1q1)\n",
    "      - [Solutions](#part1q1solution)\n",
    "  - [Question 2: Imputation Methods [45 pts]](#part1q2)\n",
    "      - [Solutions](#part1q2solution)\n",
    "\n",
    "\n",
    "- [**PART 2 [45 pts]: Principal Componant Analysis**](#part2)\n",
    "  - [Question 3: PCA for Regression [35 pts]](#part2q3)\n",
    "      - [Solutions](#part2q3solution)\n",
    "  - [Question 4: Visualizing Transformed Data [10 pts]](#part2q4)\n",
    "      - [Solutions](#part2q4solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "# PART 1 [55 pts]: Predicting the selling price of used cars\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview and Data Description \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this part, we analyze the data about used cars from a [Kaggle project](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho). The dataset is pre-processed and modified so that it contains missing values. The goal is to handle missing data and predict selling prices from the other features available in this dataset.\n",
    "\n",
    "### Dataset \n",
    "\n",
    "The training dataset is available as `data/vehicle_dataset_train.csv`. It contains the following columns:\n",
    "\n",
    "- `year` - year of the car when it was bought, \n",
    "- `mileage` - mileage of the car,\n",
    "- `max_power` - maximum power of the engine (in bhps),\n",
    "- `selling_price` - price at which the car is being sold (in lakh rupees)\n",
    "\n",
    "The testing dataset is available as `data/vehicle_dataset_test.csv`. It contains all columns mentioned above.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "We will handle missing data and predict `selling_price` from the other features available in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1q1\"></a>\n",
    "\n",
    "## <div class='exercise'><b>Question 1: Visualizing Missing Data [10 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "**Load the dataset, inspect it, and answer the following questions:**\n",
    "\n",
    "\n",
    "**1.1** How many columns and which ones include missing data in **X_train**? In each of those columns, how many observations are missing?\n",
    "\n",
    "**1.2** Plot a boxplot of `year` for all samples that contain missing values. In the same plot, generate another boxplot of `year` for all samples that do not contain missing values. Do you see any pattern?  If so, what might be the implications of that pattern?\n",
    "    \n",
    "    \n",
    "**PLEASE NOTE:** In this course, you will be expected to ALWAYS label your axes, title your graphs, and produce visuals which clearly communicate the data (as described in the [Instructions](#instructions) at the start of this notebook). Visuals should often be accompanied by text identifying the key point of the visual and defending any choices you make as a data scientist regarding the visual to best communicate your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1q1solution\"></a>\n",
    "## Question 1: Solutions \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>max_power</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>2015</td>\n",
       "      <td>17.40</td>\n",
       "      <td>117.30</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2017</td>\n",
       "      <td>13.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>2018</td>\n",
       "      <td>24.00</td>\n",
       "      <td>73.97</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2009</td>\n",
       "      <td>19.70</td>\n",
       "      <td>46.30</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>2014</td>\n",
       "      <td>16.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  mileage  max_power  selling_price\n",
       "6601  2015    17.40     117.30           70.0\n",
       "504   2017    13.60        NaN          262.5\n",
       "5812  2018    24.00      73.97           71.0\n",
       "1443  2009    19.70      46.30           15.0\n",
       "7453  2014    16.02        NaN           42.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df_train = pd.read_csv(\"data/vehicle_dataset_train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"data/vehicle_dataset_test.csv\", index_col=0)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop(columns=['selling_price']), df_train['selling_price']\n",
    "X_test, y_test = df_test.drop(columns=['selling_price']), df_test['selling_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** How many columns and which ones include missing data in **X_train**? In each of those columns, how many observations are missing?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2** Generate a boxplot of `year` for all samples that have missing values. In the same plot, generate another boxplot of `year` for all samples that do not have missing values. Do you see any pattern?  If so, what might be the implications of that pattern? \n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1q2\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 2:   Imputation Methods [45 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "We will try different ways of dealing with missing data.\n",
    "\n",
    "**2.1** First, we consider mean imputation:\n",
    "  - Use SimpleImputer to impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**. \n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "\n",
    "**2.2** Now, we will impute the data using k-NN regression model and see how it works:\n",
    "  - Use KNNImputer ($k$=2) to impute both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "\n",
    "**2.3** Now, let's examine the indicator method:\n",
    "  - For both the training and testing data, create an additional predictor called `has_missing_value` that indicates\n",
    "   whether each row has any missing value. Impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "\n",
    "\n",
    "**2.4** Discuss your results by answering the following questions.  You should answer the questions directly in the provided markdown cells of your notebook.\n",
    "\n",
    "- **2.4.1** Which method results in the largest $R^2$ value? Interpret your findings.\n",
    "\n",
    "- **2.4.2** Compare the $R^2$ values in **2.1** and **2.3**. Does adding an indicator variable help? Do these indicator method results provide any support **for** or **against** a claim that the data is missing completely at random?  Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1q2solution\"></a>\n",
    "## Question 2: Solutions \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "    \n",
    "\n",
    "**2.1** First, we consider mean imputation:\n",
    "  - Use SimpleImputer to impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**. \n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** Now, we will impute the data using k-NN regression model and see how it works:\n",
    "  - Use KNNImputer ($k$=2) to impute both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "    \n",
    "**2.3** Now, let's examine the indicator method:\n",
    "  - For both the training and testing data, create an additional predictor called `has_missing_value` that indicates\n",
    "   whether each row has any missing value. Impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and a k-NN regression model ($k$=2) on the training data. \n",
    "  - Report the $R^2$ values on the testing data for each of your two fitted models.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "    \n",
    "\n",
    "**2.4.1** Which method results in the largest $R^2$ value? Interpret your findings.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.4.2** Compare the $R^2$ values in **2.1** and **2.3**. Does adding an indicator variable help? Do these indicator method results provide any support **for** or **against** a claim that the data is missing completely at random? Why or why not?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "    \n",
    "# PART 2 [45 pts]: Principal Component Analysis\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2q3\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 3: PCA for Regression [35 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    " \n",
    "\n",
    "In this question, we will be using a dataset called \"Communities and Crime\" adapted from [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime). The modified dataset contains 122 predictor variables and 1 response variable. All numeric data was normalized into the decimal range 0.00-1.00. Some of the predictor variables are:\n",
    "\n",
    "- `householdsize`: mean people per household\n",
    "- `medIncome`: median household income\n",
    "- `PctHousOccup`: percent of housing occupied\n",
    "- `RentMedian`: rental housing - median rent\n",
    "- `PolicReqPerOffic`: total requests for police per police officer\n",
    "\n",
    "And the response variable is \n",
    "\n",
    "- `ViolentCrimesPerPop`: total number of violent crimes per 100K popuation\n",
    "\n",
    "\n",
    "\n",
    "**Load the dataset, split it in training (80%) and test set (20%), inspect it, and answer the following questions:**\n",
    "\n",
    "\n",
    "**3.1**  Compute the correlation matrix for the predictor variables in the training data (DO NOT print the entire matrix). Which pairs of distinct predictor variables have correlation greater than 0.99 or less than -0.99? (Output only those pairs with the corresponding correlation.)\n",
    "\n",
    "**3.2** Fit a linear regression model on the training data **using all available predictors**. Report and interpret the $R^2$ value on the testing data.\n",
    "\n",
    "\n",
    "**3.3** Now let's consider PCA for regression:\n",
    "  - **3.3.1** Standardize both **X_train** and **X_test** and for each number of components $k$ in $k \\in \\{1,2,3,4,5,6,8,10,12,15,20\\}$: \n",
    "  \n",
    "      - Fit the PCA transformation with n_components = $k$ on the standardized **X_train**.\n",
    "     \n",
    "      - Apply the PCA transformation to the standardized **X_train**.   \n",
    "      \n",
    "     - Use scikit-learn's cross_validate(...) to perform a 10-fold cross validation for a linear regression model on the transformed training data. \n",
    "   \n",
    "    Plot the mean validation MSE for each $k$. Report the best $k$ based on the mean validation MSE.\n",
    "          \n",
    "        \n",
    "  - **3.3.2** Now let's compute the $R^2$ value on the testing data:\n",
    "    - Fit the PCA transformation with n_components equals the best $k$ on the standardized **X_train**.\n",
    "    - Apply the PCA transformation to both the standardized **X_train** and the standardized **X_test**. \n",
    "    - Fit a linear regression model to the PCA-transformed components and report the  test $R^2$.\n",
    "    \n",
    "**3.4** Compare the $R^2$ value obtained from **3.2** and **3.3.2**. Interpret the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>numbUrban</th>\n",
       "      <th>pctUrban</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>pctWWage</th>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWSocSec</th>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <th>pctWRetire</th>\n",
       "      <th>medFamInc</th>\n",
       "      <th>perCapInc</th>\n",
       "      <th>whitePerCap</th>\n",
       "      <th>blackPerCap</th>\n",
       "      <th>indianPerCap</th>\n",
       "      <th>AsianPerCap</th>\n",
       "      <th>...</th>\n",
       "      <th>LemasSwFTFieldOps</th>\n",
       "      <th>LemasSwFTFieldPerPop</th>\n",
       "      <th>LemasTotalReq</th>\n",
       "      <th>LemasTotReqPerPop</th>\n",
       "      <th>PolicReqPerOffic</th>\n",
       "      <th>PolicPerPop</th>\n",
       "      <th>RacialMatchCommPol</th>\n",
       "      <th>PctPolicWhite</th>\n",
       "      <th>PctPolicBlack</th>\n",
       "      <th>PctPolicHisp</th>\n",
       "      <th>PctPolicAsian</th>\n",
       "      <th>PctPolicMinor</th>\n",
       "      <th>OfficAssgnDrugUnits</th>\n",
       "      <th>NumKindsDrugsSeiz</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.15           0.31          0.40          0.63          0.14   \n",
       "2        0.25           0.54          0.05          0.71          0.48   \n",
       "3        1.00           0.42          0.47          0.59          0.12   \n",
       "4        0.11           0.43          0.04          0.89          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  numbUrban  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32       0.20   \n",
       "1         0.06         0.58         0.72         0.65        0.47       0.16   \n",
       "2         0.30         0.42         0.48         0.28        0.32       0.26   \n",
       "3         0.05         0.41         0.53         0.34        0.33       1.00   \n",
       "4         0.06         0.45         0.48         0.31        0.46       0.13   \n",
       "\n",
       "   pctUrban  medIncome  pctWWage  pctWFarmSelf  pctWInvInc  pctWSocSec  \\\n",
       "0      1.00       0.37      0.72          0.34        0.60        0.29   \n",
       "1      1.00       0.22      0.52          0.10        0.51        0.48   \n",
       "2      1.00       0.33      0.55          0.37        0.37        0.39   \n",
       "3      0.99       0.28      0.62          0.16        0.36        0.40   \n",
       "4      1.00       0.22      0.52          0.44        0.49        0.56   \n",
       "\n",
       "   pctWPubAsst  pctWRetire  medFamInc  perCapInc  whitePerCap  blackPerCap  \\\n",
       "0         0.15        0.43       0.39       0.40         0.39         0.32   \n",
       "1         0.39        0.51       0.30       0.29         0.34         0.23   \n",
       "2         0.64        0.44       0.32       0.29         0.32         0.23   \n",
       "3         0.30        0.45       0.29       0.30         0.35         0.21   \n",
       "4         0.41        0.39       0.25       0.25         0.25         0.16   \n",
       "\n",
       "   indianPerCap  AsianPerCap  ...  LemasSwFTFieldOps  LemasSwFTFieldPerPop  \\\n",
       "0          0.27         0.27  ...               0.96                  0.17   \n",
       "1          0.13         0.20  ...               0.93                  0.38   \n",
       "2          0.17         0.17  ...               0.96                  0.12   \n",
       "3          0.20         0.33  ...               0.75                  0.19   \n",
       "4          0.07         0.20  ...               0.98                  0.14   \n",
       "\n",
       "   LemasTotalReq  LemasTotReqPerPop  PolicReqPerOffic  PolicPerPop  \\\n",
       "0           0.06               0.18              0.44         0.13   \n",
       "1           0.05               0.21              0.23         0.30   \n",
       "2           0.05               0.11              0.35         0.08   \n",
       "3           0.35               0.19              0.38         0.16   \n",
       "4           0.03               0.14              0.37         0.11   \n",
       "\n",
       "   RacialMatchCommPol  PctPolicWhite  PctPolicBlack  PctPolicHisp  \\\n",
       "0                0.94           0.93           0.03          0.07   \n",
       "1                0.61           0.89           0.15          0.01   \n",
       "2                0.80           0.82           0.04          0.19   \n",
       "3                0.82           0.70           0.45          0.03   \n",
       "4                0.84           0.96           0.00          0.00   \n",
       "\n",
       "   PctPolicAsian  PctPolicMinor  OfficAssgnDrugUnits  NumKindsDrugsSeiz  \\\n",
       "0           0.10           0.07                 0.02               0.57   \n",
       "1           0.06           0.12                 0.10               0.64   \n",
       "2           0.19           0.18                 0.05               0.57   \n",
       "3           0.05           0.33                 0.13               0.57   \n",
       "4           0.00           0.00                 0.02               0.86   \n",
       "\n",
       "   PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
       "0              0.29      0.12     0.26            0.20       0.06   \n",
       "1              0.22      0.06     0.39            0.84       0.06   \n",
       "2              0.36      0.09     0.46            0.05       0.09   \n",
       "3              1.00      1.00     0.07            0.15       1.00   \n",
       "4              0.29      0.16     0.12            0.07       0.04   \n",
       "\n",
       "   PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0           0.04                 0.90                  0.5   \n",
       "1           0.06                 0.91                  0.5   \n",
       "2           0.05                 0.88                  0.5   \n",
       "3           0.35                 0.73                  0.0   \n",
       "4           0.01                 0.81                  1.0   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0                 0.32             0.14                 0.20  \n",
       "1                 0.88             0.26                 0.49  \n",
       "2                 0.76             0.13                 0.34  \n",
       "3                 0.31             0.21                 0.69  \n",
       "4                 0.56             0.09                 0.63  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df = pd.read_csv(\"data/communities_and_crime.csv\", index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data\n",
    "X,y = df.drop(columns=['ViolentCrimesPerPop']), df['ViolentCrimesPerPop']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2q3solution\"></a>\n",
    "## Question 3: Solutions \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.1** Compute the correlation matrix for the predictor variables in the training data (DO NOT print the entire matrix). Which pairs of distinct predictor variables have correlation greater than 0.99 or less than -0.99? (Output only those pairs with the corresponding correlation.)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Fit a linear regression model on the training data **using all available predictors**. Report and interpret the $R^2$ value on the testing data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "\n",
    "**3.3.1** Standardize both **X_train** and **X_test**,  and for each number of components $k$ in $k \\in \\{1,2,3,4,5,6,8,10,12,15,20\\}$: \n",
    "\n",
    "  - Fit the PCA transformation with n_components = $k$ on the standardized **X_train**.\n",
    "    \n",
    "  - Apply the PCA transformation to the standardized **X_train**.\n",
    "    \n",
    "  - Use scikit-learn's cross_validate(...) to perform a 10-fold cross validation for a linear regression model on the transformed training data. \n",
    "    \n",
    "  Plot the mean validation MSE for each $k$. Report the best $k$ based on the mean validation MSE.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "    \n",
    "\n",
    "**3.3.2** Now let's compute the $R^2$ value on the testing data:\n",
    "  - Fit the PCA transformation with n_components equals the best $k$ on the standardized **X_train**.\n",
    "  - Apply the PCA transformation to the standardized **X_train** and the standardized **X_test**. \n",
    "  - Fit a linear regression model to the PCA-transformed components and report the  test $R^2$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.4** Compare the $R^2$ value obtained from **3.2** and **3.3.2**. Interpret the result.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2q4\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 4: Visualizing Transformed Data [10 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, we will be using a dataset called \"Better Life Index\" adapted from [Organisation for Economic Co-operation and Development](https://stats.oecd.org/). The modified dataset contains 24 numerical variables and 1 categorical variable. The categorical variable `Country` is the name of the country. Some of the numerical variables include:\n",
    "\n",
    "- `Dwellings without basic facilities`\n",
    "- `Housing expenditure`\n",
    "- `Rooms per person`\n",
    "- `Household net adjusted disposable income`\n",
    "- `Household net financial wealth`\n",
    "- ...\n",
    "\n",
    "\n",
    "**Load the dataset, and answer the following questions:**\n",
    "\n",
    "\n",
    "**4.1** Standardize **X** and apply PCA transformation with n_components = 2 to your standardized data.\n",
    "\n",
    "**4.2** Make a scatter plot for the transformed data, where the x-axis corresponds to the first principal component, and the y-axis corresponds to the second principal component. Label each point by its corresponding country name. Do you observe any pattern in the scatter plot?\n",
    "\n",
    "**4.3** In Question 3, where we also used PCA, we had a training and a test set. Here we do not split the data. Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df = pd.read_csv(\"data/OECD_well-being.csv\", index_col = 0)\n",
    "country, X = df['Country'], df.drop(columns='Country').values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2q4solution\"></a>\n",
    "## Question 4: Solutions \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.1** Standardize **X** and apply PCA transformation with n_components = 2 to your standardized data.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** Make a scatter plot for the transformed data, where the x-axis corresponds to the first principal component, and the y-axis corresponds to the second principal component. Label each point by its corresponding country name. Do you observe any pattern in the scatter plot?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.3** In Question 3, where we also used PCA, we had a training and a test set. Here we do not split the data. Explain why.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
